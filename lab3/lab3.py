# -*- coding: utf-8 -*-
"""lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h_WOl3X97fua1xFP3S8SOs3KMQ0Odzlz
"""

import numpy as np
from PIL import Image, ImageOps
import os
import matplotlib.pyplot as plt
from google.colab import drive
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from keras.datasets import mnist
drive.mount('/content/drive')
!ls "/content/drive/My Drive/"

(x_train_full, y_train_full), (x_test_full, y_test_full) = mnist.load_data()

classA = 1
classB = 0

train_filter = np.where((y_train_full == classA) | (y_train_full == classB))
test_filter = np.where((y_test_full == classA) | (y_test_full == classB))

x_train, y_train = x_train_full[train_filter], y_train_full[train_filter]
x_test, y_test = x_test_full[test_filter], y_test_full[test_filter]

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

plt.figure(figsize = (15, 7))
for i in range(27):
  plt.subplot(3, 9, i + 1)
  plt.title("Number {0}".format(y_train[i]))
  plt.imshow(x_train[i], cmap = "gray")

x_train = np.array(x_train, np.float32)
x_test = np.array(x_train, np.float32)

x_train = x_train.reshape([-1, 784])
x_test = x_test.reshape([-1, 784])

x_train = x_train / 255.0
x_test = x_test / 255.0

pca = PCA(n_components = 2)
pca_reduce = pca.fit_transform(x_train)

pca_x1 = pca_reduce[:, 0]
pca_x2 = pca_reduce[:, 1]

plt.scatter(pca_x1, pca_x2, s = 50, c = y_train, alpha = 0.5)
plt.show()

print(x)
pca_x1 = x[:, 0]
pca_x2 = x[:, 1]

plt.scatter(pca_x1, pca_x2, s = 50, c = y, alpha = 0.5)
plt.show()

tsne = TSNE(n_components = 2, verbose = 1, perplexity = 35, n_iter = 250)
tsne_result = tsne.fit_transform(x_train)

tsne_x1 = tsne_result[:, 0]
tsne_x2 = tsne_result[:, 1]

plt.figure(figsize = (16, 10))
plt.scatter(tsne_x1, tsne_x2, s = 5, c = y_train, alpha = 0.5)
plt.show()

class SVM:
  def __init__(self, kernel='linear', C=10000.0, max_iter=100000, degree=3, gamma=1):
    self.kernel = {'poly'  : lambda x,y: np.dot(x, y.T)**degree,
         'rbf': lambda x,y: np.exp(-gamma*np.sum((y-x[:,np.newaxis])**2,axis=-1)),
         'linear': lambda x,y: np.dot(x, y.T)}[kernel]
    self.C = C
    self.max_iter = max_iter
    self.y = []
    self.x = []

  # ограничение параметра t, чтобы новые лямбды не покидали границ квадрата
  def restrict_to_square(self, t, v0, u): 
    t = (np.clip(v0 + t*u, 0, self.C) - v0)[1]/u[1]
    return (np.clip(v0 + t*u, 0, self.C) - v0)[0]/u[0]

  def fit(self, X, y):
    self.X = X.copy()
    # преобразование классов 0,1 в -1,+1; для лучшей совместимости с sklearn
    self.y = y
    self.lambdas = np.zeros_like(self.y, dtype=float)
    # формула (3)
    self.K = self.kernel(self.X, self.X) * self.y[:,np.newaxis] * self.y
    
    # выполняем self.max_iter итераций
    for _ in range(self.max_iter):
      # проходим по всем лямбда 
      for idxM in range(len(self.lambdas)):                                    
        # idxL выбираем случайно
        idxL = np.random.randint(0, len(self.lambdas))                         
        # формула (4с)
        Q = self.K[[[idxM, idxM], [idxL, idxL]], [[idxM, idxL], [idxM, idxL]]] 
        # формула (4a)
        v0 = self.lambdas[[idxM, idxL]]                                        
        # формула (4b)
        k0 = 1 - np.sum(self.lambdas * self.K[[idxM, idxL]], axis=1)           
        # формула (4d)
        u = np.array([-self.y[idxL], self.y[idxM]])                            
        # регуляризированная формула (5), регуляризация только для idxM = idxL
        t_max = np.dot(k0, u) / (np.dot(np.dot(Q, u), u) + 1E-15) 
        self.lambdas[[idxM, idxL]] = v0 + u * self.restrict_to_square(t_max, v0, u)
    
    # найти индексы опорных векторов
    idx, = np.nonzero(self.lambdas > 1E-15) 
    # формула (1)
    self.b = np.mean((1.0-np.sum(self.K[idx]*self.lambdas, axis=1))*self.y[idx]) 
  
  def decision_function(self, X):
    return np.sum(self.kernel(X, self.X) * self.y * self.lambdas, axis=1) + self.b

  def predict(self, X): 
    # преобразование классов -1,+1 в 0,1; для лучшей совместимости с sklearn
    return (np.sign(self.decision_function(X)) + 1) // 2

svm = SVM(kernel = 'linear', C = 20, max_iter=1000)
svm.fit(x_train, y_train)

accuracy = 0
for i in range(len(y_test)):
  answer = svm.predict([x_test[i]])[0]
  print(answer," : ",y_test[i])
  if answer == y_test[i]: accuracy += 1
accuracy /= len(y_test)
print(accuracy)

svm = SVM()
svm.fit(x_train, y_train)
accuracy = 0
for i in range(len(y_test)):
  answer = svm.predict([x_test[i]])[0]
  print(answer," : ",y_test[i])
  if answer == y_test[i]: accuracy += 1
accuracy /= len(y_test)
print('accuracy: ', accuracy)

kernels = ['linear', 'poly', 'rbf', 'sigmoid']
for k in kernels:
  clf = svm.SVC(kernel=k)
  clf.fit(x_train, y_train)
  print('Kernel: ', k)
  accuracy = 0
  for i in range(len(y_test)):
    answer = clf.predict([x_test[i]])[0]
    print(answer," : ",y_test[i])
    if answer == y_test[i]: accuracy += 1
  accuracy /= len(y_test)
  print('accuracy: ', accuracy)